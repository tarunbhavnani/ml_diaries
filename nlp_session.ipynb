{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMEcorner NLP Session1- Text Classification\n",
    "\n",
    "Objectives:\n",
    "\n",
    "    - Machines understanding of text\n",
    "    \n",
    "    - Basic text cleaning\n",
    "    \n",
    "    - basic to advanced text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification is a Supervised Predictive modelling exercise.\n",
    "### To understand text classification lets first look at a ml classification example. \n",
    "### Here we will try and see how we will classify the flowers types in the famous IRIS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = load_iris()\n",
    "iris_df=pd.DataFrame(iris['data'],  columns= iris['feature_names'])\n",
    "iris_df['labels']= iris['target']\n",
    "#'setosa', 'versicolor', 'virginica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "68                 6.2               2.2                4.5               1.5   \n",
       "14                 5.8               4.0                1.2               0.2   \n",
       "123                6.3               2.7                4.9               1.8   \n",
       "18                 5.7               3.8                1.7               0.3   \n",
       "114                5.8               2.8                5.1               2.4   \n",
       "\n",
       "     labels  \n",
       "68        1  \n",
       "14        0  \n",
       "123       2  \n",
       "18        0  \n",
       "114       2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELECTROBOT\\Anaconda3\\envs\\tarun37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initiate logistic regreesion\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "#fit our parameters\n",
    "clf.fit(iris_df.iloc[:, 0:4].values, iris_df.iloc[:, 4].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pridict flower for sepal length (cm)=8, sepal width (cm)= 4, petal length (cm)=7, petal width (cm)=2\n",
    "\n",
    "check_candidate=([[8,4,7,2]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(check_candidate)[0] #'virginica'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the few lines of code above, we created a classification algorithm for the iris dataset.\n",
    "\n",
    "### How do you think the text classification differs from this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets first load and understand our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48905</th>\n",
       "      <td>4</td>\n",
       "      <td>Delaware to see Oracle-PeopleSoft (TheDeal.com)</td>\n",
       "      <td>TheDeal.com - The takeover fight moves to Dela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53455</th>\n",
       "      <td>2</td>\n",
       "      <td>Braves look to tie up series</td>\n",
       "      <td>Mike Hampton has never won a Division Series g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116217</th>\n",
       "      <td>4</td>\n",
       "      <td>Group Releases Mouse Brain Genome Data (AP)</td>\n",
       "      <td>AP - The Allen Institute for Brain Science has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2</td>\n",
       "      <td>Sheffield Should Be Able to Play On</td>\n",
       "      <td>The Yankees' Gary Sheffield has a pulled muscl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111350</th>\n",
       "      <td>1</td>\n",
       "      <td>Court Overturns Nigerian Woman's Stoning Sentence</td>\n",
       "      <td>NINGI, Nigeria (Reuters) - A young Nigerian m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70265</th>\n",
       "      <td>1</td>\n",
       "      <td>Mexican biologist's attempt to slip into US en...</td>\n",
       "      <td>SASABE, Mexico -- Mario Alberto Diaz, a biolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81775</th>\n",
       "      <td>2</td>\n",
       "      <td>Undefeated Diaz retains title</td>\n",
       "      <td>Juan Diaz carried his impressive boxing show o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21281</th>\n",
       "      <td>1</td>\n",
       "      <td>Anwar makes  #39;amazing #39; recovery</td>\n",
       "      <td>The newly released former deputy of Malaysia, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110393</th>\n",
       "      <td>4</td>\n",
       "      <td>Apple tangles with owner of itunes.co.uk (MacC...</td>\n",
       "      <td>MacCentral - U.K.-based Internet entrepreneur ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30734</th>\n",
       "      <td>2</td>\n",
       "      <td>HOPKINS SETS THE PACE</td>\n",
       "      <td>John Hopkins caused a surprise by claiming pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36650</th>\n",
       "      <td>4</td>\n",
       "      <td>Memory makers face jail for price fixing</td>\n",
       "      <td>THREE German executives and an American with m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15703</th>\n",
       "      <td>1</td>\n",
       "      <td>British Based Preacher Probed over Child Traff...</td>\n",
       "      <td>A Britain-based preacher who claims to perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9320</th>\n",
       "      <td>2</td>\n",
       "      <td>Aussies face Cuba in final</td>\n",
       "      <td>Brendan Kingman's sixth-inning RBI single sent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61541</th>\n",
       "      <td>3</td>\n",
       "      <td>Airbus to Deliver as Many as 320 Planes in 200...</td>\n",
       "      <td>Airbus SAS, the world #39;s biggest planemaker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82786</th>\n",
       "      <td>2</td>\n",
       "      <td>Hendrick Plane Crash Preliminary Report</td>\n",
       "      <td>A preliminary report, out Friday, suggests the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24033</th>\n",
       "      <td>3</td>\n",
       "      <td>BREAKING NEWS  Federal judge rejects US bid to...</td>\n",
       "      <td>CNET is reporting that US District Judge Vaugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75788</th>\n",
       "      <td>4</td>\n",
       "      <td>Titan #39;s Big Surprise</td>\n",
       "      <td>The Cassini flyby of Titan sent back the most ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>4</td>\n",
       "      <td>Science calls for anglers to cut line</td>\n",
       "      <td>All those recreational anglers who dangle bait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60727</th>\n",
       "      <td>3</td>\n",
       "      <td>Europe and Health Care Costs Hurt G.M. Earnings</td>\n",
       "      <td>General Motors lowered its forecast for the en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84176</th>\n",
       "      <td>1</td>\n",
       "      <td>Nuclear summit opens in Australia</td>\n",
       "      <td>Asia-Pacific leaders meet in Australia to disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110661</th>\n",
       "      <td>2</td>\n",
       "      <td>Klitschko feels weight of a nation</td>\n",
       "      <td>With a laugh Vitali Klitschko admits he will n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52162</th>\n",
       "      <td>2</td>\n",
       "      <td>Reds' Vander Wal Becomes Free Agent (AP)</td>\n",
       "      <td>AP - Pinch-hitter John Vander Wal chose free a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8485</th>\n",
       "      <td>3</td>\n",
       "      <td>US Airways, pilots union break off talks</td>\n",
       "      <td>WASHINGTON -- US Airways Group Inc. and its pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52095</th>\n",
       "      <td>3</td>\n",
       "      <td>Computer Associates to buy Netegrity</td>\n",
       "      <td>Computer Associates in Islandia, NY, said Wedn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80348</th>\n",
       "      <td>1</td>\n",
       "      <td>Car bombs kill 12; US readies attack</td>\n",
       "      <td>Car bombs killed at least a dozen people in Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51573</th>\n",
       "      <td>1</td>\n",
       "      <td>Israel diverts airliner to Cyprus in bomb scare</td>\n",
       "      <td>TEL AVIV -- Israeli fighter planes escorted a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29964</th>\n",
       "      <td>4</td>\n",
       "      <td>Glimpse at Swollen Stars Presages Earth's Demi...</td>\n",
       "      <td>SPACE.com - Today's global warming nbsp;is not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100431</th>\n",
       "      <td>4</td>\n",
       "      <td>Game should be pulled</td>\n",
       "      <td>I was just shy of a year and a half old when J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92583</th>\n",
       "      <td>1</td>\n",
       "      <td>Sudan Rebels Hand Over 20 Prisoners of War</td>\n",
       "      <td>DERIBAT, Sudan (Reuters) - Darfur rebels from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9177</th>\n",
       "      <td>3</td>\n",
       "      <td>Gateway Adds Micro Center to Its Distribution ...</td>\n",
       "      <td>NewsFactor - Gateway has added another marquee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20762</th>\n",
       "      <td>3</td>\n",
       "      <td>Cephalon subpoenaed by US attorney</td>\n",
       "      <td>Drugmaker Cephalon Inc. said Tuesday it receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74572</th>\n",
       "      <td>4</td>\n",
       "      <td>Are Cheaper Flat-Panel TVs On The Way?</td>\n",
       "      <td>IFire aims to displace LCD TVs with its lower-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54048</th>\n",
       "      <td>2</td>\n",
       "      <td>Controversy may be just what NASCAR needs</td>\n",
       "      <td>The tornado that was Talladega has swept up th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91823</th>\n",
       "      <td>3</td>\n",
       "      <td>CalPERS goes after  #39;egregious #39; exec pay</td>\n",
       "      <td>The California Public Employees #39; Retiremen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57922</th>\n",
       "      <td>2</td>\n",
       "      <td>A clashing combination</td>\n",
       "      <td>David Ortiz, still champagne-scented, was watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23475</th>\n",
       "      <td>2</td>\n",
       "      <td>A dash of novelty to add to excitement</td>\n",
       "      <td>LONDON, SEPTEMBER 9: A dash of novelty would m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96792</th>\n",
       "      <td>2</td>\n",
       "      <td>A Baseball Legend Playing a New Position: Flaw...</td>\n",
       "      <td>Jackie Robinson, the man who broke baseball's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113786</th>\n",
       "      <td>1</td>\n",
       "      <td>Bomb scare at Madrid stadium</td>\n",
       "      <td>Spectators are evacuated from Real Madrid's Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28426</th>\n",
       "      <td>3</td>\n",
       "      <td>U.S. Stocks Open Lower, Coca-Cola Weighs</td>\n",
       "      <td>NEW YORK (Reuters) - U.S. stocks opened lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77805</th>\n",
       "      <td>2</td>\n",
       "      <td>Bison Changing QBs</td>\n",
       "      <td>After Marcos Moreno threw four more intercepti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>4</td>\n",
       "      <td>Microsoft Integration Server on Deck</td>\n",
       "      <td>Microsoft (Quote, Chart) is ready to ship Host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19406</th>\n",
       "      <td>1</td>\n",
       "      <td>India-Pakistan Foreign Minister Talks End Monday</td>\n",
       "      <td>NEW DELHI (Reuters) - India and Pakistan trie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8817</th>\n",
       "      <td>4</td>\n",
       "      <td>Tough for Courts to Intervene in Offensive Int...</td>\n",
       "      <td>By RACHEL KONRAD    SAN JOSE, Calif. (AP) -- Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27277</th>\n",
       "      <td>2</td>\n",
       "      <td>Andreychuk Back With Bolts</td>\n",
       "      <td>TAMPA - The Lightning have signed captain Dave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15011</th>\n",
       "      <td>4</td>\n",
       "      <td>FrontBridge Acquires MessageRite for Message A...</td>\n",
       "      <td>FrontBridge Technologies today announced the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20070</th>\n",
       "      <td>3</td>\n",
       "      <td>Update 4: Alitalia Reportedly to Cut 5,000 Jobs</td>\n",
       "      <td>Its future at stake, Italy #39;s flagship air ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19221</th>\n",
       "      <td>1</td>\n",
       "      <td>Frances Expected to Cost Insurers  #36;2B- #36...</td>\n",
       "      <td>AP - Hurricanes Charley and Frances dealt Flor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16858</th>\n",
       "      <td>4</td>\n",
       "      <td>Ballmer Touts Microsoft #39;s Innovation In Sp...</td>\n",
       "      <td>Microsoft CEO Steve Ballmer said innovation an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19376</th>\n",
       "      <td>2</td>\n",
       "      <td>Rugby-Super 12, Tri-nations to be expanded in ...</td>\n",
       "      <td>The southern hemisphere #39;s Tri-nations and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31943</th>\n",
       "      <td>3</td>\n",
       "      <td>Mortgage fraud near  #39;epidemic #39;</td>\n",
       "      <td>SAN FRANCISCO (CBS.MW) -- The US real-estate b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels                                              title  \\\n",
       "48905        4    Delaware to see Oracle-PeopleSoft (TheDeal.com)   \n",
       "53455        2                       Braves look to tie up series   \n",
       "116217       4        Group Releases Mouse Brain Genome Data (AP)   \n",
       "2157         2                Sheffield Should Be Able to Play On   \n",
       "111350       1  Court Overturns Nigerian Woman's Stoning Sentence   \n",
       "70265        1  Mexican biologist's attempt to slip into US en...   \n",
       "81775        2                      Undefeated Diaz retains title   \n",
       "21281        1             Anwar makes  #39;amazing #39; recovery   \n",
       "110393       4  Apple tangles with owner of itunes.co.uk (MacC...   \n",
       "30734        2                              HOPKINS SETS THE PACE   \n",
       "36650        4           Memory makers face jail for price fixing   \n",
       "15703        1  British Based Preacher Probed over Child Traff...   \n",
       "9320         2                         Aussies face Cuba in final   \n",
       "61541        3  Airbus to Deliver as Many as 320 Planes in 200...   \n",
       "82786        2            Hendrick Plane Crash Preliminary Report   \n",
       "24033        3  BREAKING NEWS  Federal judge rejects US bid to...   \n",
       "75788        4                           Titan #39;s Big Surprise   \n",
       "11835        4              Science calls for anglers to cut line   \n",
       "60727        3    Europe and Health Care Costs Hurt G.M. Earnings   \n",
       "84176        1                  Nuclear summit opens in Australia   \n",
       "110661       2                 Klitschko feels weight of a nation   \n",
       "52162        2           Reds' Vander Wal Becomes Free Agent (AP)   \n",
       "8485         3           US Airways, pilots union break off talks   \n",
       "52095        3               Computer Associates to buy Netegrity   \n",
       "80348        1               Car bombs kill 12; US readies attack   \n",
       "51573        1    Israel diverts airliner to Cyprus in bomb scare   \n",
       "29964        4  Glimpse at Swollen Stars Presages Earth's Demi...   \n",
       "100431       4                              Game should be pulled   \n",
       "92583        1         Sudan Rebels Hand Over 20 Prisoners of War   \n",
       "9177         3  Gateway Adds Micro Center to Its Distribution ...   \n",
       "20762        3                 Cephalon subpoenaed by US attorney   \n",
       "74572        4             Are Cheaper Flat-Panel TVs On The Way?   \n",
       "54048        2          Controversy may be just what NASCAR needs   \n",
       "91823        3    CalPERS goes after  #39;egregious #39; exec pay   \n",
       "57922        2                             A clashing combination   \n",
       "23475        2             A dash of novelty to add to excitement   \n",
       "96792        2  A Baseball Legend Playing a New Position: Flaw...   \n",
       "113786       1                       Bomb scare at Madrid stadium   \n",
       "28426        3           U.S. Stocks Open Lower, Coca-Cola Weighs   \n",
       "77805        2                                 Bison Changing QBs   \n",
       "1879         4               Microsoft Integration Server on Deck   \n",
       "19406        1   India-Pakistan Foreign Minister Talks End Monday   \n",
       "8817         4  Tough for Courts to Intervene in Offensive Int...   \n",
       "27277        2                         Andreychuk Back With Bolts   \n",
       "15011        4  FrontBridge Acquires MessageRite for Message A...   \n",
       "20070        3    Update 4: Alitalia Reportedly to Cut 5,000 Jobs   \n",
       "19221        1  Frances Expected to Cost Insurers  #36;2B- #36...   \n",
       "16858        4  Ballmer Touts Microsoft #39;s Innovation In Sp...   \n",
       "19376        2  Rugby-Super 12, Tri-nations to be expanded in ...   \n",
       "31943        3             Mortgage fraud near  #39;epidemic #39;   \n",
       "\n",
       "                                              description  \n",
       "48905   TheDeal.com - The takeover fight moves to Dela...  \n",
       "53455   Mike Hampton has never won a Division Series g...  \n",
       "116217  AP - The Allen Institute for Brain Science has...  \n",
       "2157    The Yankees' Gary Sheffield has a pulled muscl...  \n",
       "111350   NINGI, Nigeria (Reuters) - A young Nigerian m...  \n",
       "70265   SASABE, Mexico -- Mario Alberto Diaz, a biolog...  \n",
       "81775   Juan Diaz carried his impressive boxing show o...  \n",
       "21281   The newly released former deputy of Malaysia, ...  \n",
       "110393  MacCentral - U.K.-based Internet entrepreneur ...  \n",
       "30734   John Hopkins caused a surprise by claiming pro...  \n",
       "36650   THREE German executives and an American with m...  \n",
       "15703   A Britain-based preacher who claims to perform...  \n",
       "9320    Brendan Kingman's sixth-inning RBI single sent...  \n",
       "61541   Airbus SAS, the world #39;s biggest planemaker...  \n",
       "82786   A preliminary report, out Friday, suggests the...  \n",
       "24033   CNET is reporting that US District Judge Vaugh...  \n",
       "75788   The Cassini flyby of Titan sent back the most ...  \n",
       "11835   All those recreational anglers who dangle bait...  \n",
       "60727   General Motors lowered its forecast for the en...  \n",
       "84176   Asia-Pacific leaders meet in Australia to disc...  \n",
       "110661  With a laugh Vitali Klitschko admits he will n...  \n",
       "52162   AP - Pinch-hitter John Vander Wal chose free a...  \n",
       "8485    WASHINGTON -- US Airways Group Inc. and its pi...  \n",
       "52095   Computer Associates in Islandia, NY, said Wedn...  \n",
       "80348   Car bombs killed at least a dozen people in Ba...  \n",
       "51573   TEL AVIV -- Israeli fighter planes escorted a ...  \n",
       "29964   SPACE.com - Today's global warming nbsp;is not...  \n",
       "100431  I was just shy of a year and a half old when J...  \n",
       "92583    DERIBAT, Sudan (Reuters) - Darfur rebels from...  \n",
       "9177    NewsFactor - Gateway has added another marquee...  \n",
       "20762   Drugmaker Cephalon Inc. said Tuesday it receiv...  \n",
       "74572   IFire aims to displace LCD TVs with its lower-...  \n",
       "54048   The tornado that was Talladega has swept up th...  \n",
       "91823   The California Public Employees #39; Retiremen...  \n",
       "57922   David Ortiz, still champagne-scented, was watc...  \n",
       "23475   LONDON, SEPTEMBER 9: A dash of novelty would m...  \n",
       "96792   Jackie Robinson, the man who broke baseball's ...  \n",
       "113786  Spectators are evacuated from Real Madrid's Be...  \n",
       "28426    NEW YORK (Reuters) - U.S. stocks opened lower...  \n",
       "77805   After Marcos Moreno threw four more intercepti...  \n",
       "1879    Microsoft (Quote, Chart) is ready to ship Host...  \n",
       "19406    NEW DELHI (Reuters) - India and Pakistan trie...  \n",
       "8817    By RACHEL KONRAD    SAN JOSE, Calif. (AP) -- Y...  \n",
       "27277   TAMPA - The Lightning have signed captain Dave...  \n",
       "15011   FrontBridge Technologies today announced the a...  \n",
       "20070   Its future at stake, Italy #39;s flagship air ...  \n",
       "19221   AP - Hurricanes Charley and Frances dealt Flor...  \n",
       "16858   Microsoft CEO Steve Ballmer said innovation an...  \n",
       "19376   The southern hemisphere #39;s Tri-nations and ...  \n",
       "31943   SAN FRANCISCO (CBS.MW) -- The US real-estate b...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat= pd.read_csv(r'C:\\Users\\ELECTROBOT\\Desktop\\nlp_session\\news\\train.csv')\n",
    "\n",
    "dat.columns= ['labels', 'title', 'description']\n",
    "dat.sample(frac=1).head(50)\n",
    "#'World','Sports','Business','Sci/Tech'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not by alot.\n",
    "\n",
    "### The features vector above were petal, sepal length and width. \n",
    "\n",
    "### In text classification we have to create a feature vector from our documents(sentences)\n",
    "\n",
    "### Words are our features.\n",
    "\n",
    "### Now we will see the different ways we convert words to features, also called text vectorization methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the titles and train the model to predict the lables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: How does machine understand language\n",
    "\n",
    "\n",
    "We have to classifiy the sentences. Every word possible is a feature and the count of words in the sentence makes the feature vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................................."
     ]
    }
   ],
   "source": [
    "#Frequency of words\n",
    "words={}\n",
    "for sent in dat.title.values:\n",
    "    for word in sent.split():\n",
    "        if word in words:\n",
    "            words[word]+=1\n",
    "        else:\n",
    "            words[word]=1\n",
    "# Creating a term document matrix, we will sue just the first 100 observations\n",
    "final_array=[len([i for i in dat.iloc[0].title.split() if i==word]) for word in words]\n",
    "for sent in dat[1:100].title.values:\n",
    "    print('.', end=\"\", flush=True)\n",
    "    ary= [len([i for i in sent.split() if i==word]) for word in words]\n",
    "    final_array= np.vstack((final_array, ary))\n",
    "\n",
    "    \n",
    "tdm=pd.DataFrame(final_array, columns= words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wall</th>\n",
       "      <th>St.</th>\n",
       "      <th>Bears</th>\n",
       "      <th>Claw</th>\n",
       "      <th>Back</th>\n",
       "      <th>Into</th>\n",
       "      <th>the</th>\n",
       "      <th>Black</th>\n",
       "      <th>(Reuters)</th>\n",
       "      <th>Carlyle</th>\n",
       "      <th>...</th>\n",
       "      <th>Mega-Deals,</th>\n",
       "      <th>season-opener</th>\n",
       "      <th>Cost-Cutters</th>\n",
       "      <th>Redeploys</th>\n",
       "      <th>Quizzing</th>\n",
       "      <th>'offensive'</th>\n",
       "      <th>Barack</th>\n",
       "      <th>Obama</th>\n",
       "      <th>Rauffer</th>\n",
       "      <th>Shivering</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Wall St. Bears Claw Back Into the Black (Reuters)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carlyle Looks Toward Commercial Aerospace (Reuters)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oil and Economy Cloud Stocks' Outlook (Reuters)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oil prices soar to all-time record, posing new menace to US economy (AFP)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71744 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Wall  St.  Bears  Claw  \\\n",
       "title                                                                        \n",
       "Wall St. Bears Claw Back Into the Black (Reuters)      1    1      1     1   \n",
       "Carlyle Looks Toward Commercial Aerospace (Reut...     0    0      0     0   \n",
       "Oil and Economy Cloud Stocks' Outlook (Reuters)        0    0      0     0   \n",
       "Iraq Halts Oil Exports from Main Southern Pipel...     0    0      0     0   \n",
       "Oil prices soar to all-time record, posing new ...     0    0      0     0   \n",
       "\n",
       "                                                    Back  Into  the  Black  \\\n",
       "title                                                                        \n",
       "Wall St. Bears Claw Back Into the Black (Reuters)      1     1    1      1   \n",
       "Carlyle Looks Toward Commercial Aerospace (Reut...     0     0    0      0   \n",
       "Oil and Economy Cloud Stocks' Outlook (Reuters)        0     0    0      0   \n",
       "Iraq Halts Oil Exports from Main Southern Pipel...     0     0    0      0   \n",
       "Oil prices soar to all-time record, posing new ...     0     0    0      0   \n",
       "\n",
       "                                                    (Reuters)  Carlyle  ...  \\\n",
       "title                                                                   ...   \n",
       "Wall St. Bears Claw Back Into the Black (Reuters)           1        0  ...   \n",
       "Carlyle Looks Toward Commercial Aerospace (Reut...          1        1  ...   \n",
       "Oil and Economy Cloud Stocks' Outlook (Reuters)             1        0  ...   \n",
       "Iraq Halts Oil Exports from Main Southern Pipel...          1        0  ...   \n",
       "Oil prices soar to all-time record, posing new ...          0        0  ...   \n",
       "\n",
       "                                                    Mega-Deals,  \\\n",
       "title                                                             \n",
       "Wall St. Bears Claw Back Into the Black (Reuters)             0   \n",
       "Carlyle Looks Toward Commercial Aerospace (Reut...            0   \n",
       "Oil and Economy Cloud Stocks' Outlook (Reuters)               0   \n",
       "Iraq Halts Oil Exports from Main Southern Pipel...            0   \n",
       "Oil prices soar to all-time record, posing new ...            0   \n",
       "\n",
       "                                                    season-opener  \\\n",
       "title                                                               \n",
       "Wall St. Bears Claw Back Into the Black (Reuters)               0   \n",
       "Carlyle Looks Toward Commercial Aerospace (Reut...              0   \n",
       "Oil and Economy Cloud Stocks' Outlook (Reuters)                 0   \n",
       "Iraq Halts Oil Exports from Main Southern Pipel...              0   \n",
       "Oil prices soar to all-time record, posing new ...              0   \n",
       "\n",
       "                                                    Cost-Cutters  Redeploys  \\\n",
       "title                                                                         \n",
       "Wall St. Bears Claw Back Into the Black (Reuters)              0          0   \n",
       "Carlyle Looks Toward Commercial Aerospace (Reut...             0          0   \n",
       "Oil and Economy Cloud Stocks' Outlook (Reuters)                0          0   \n",
       "Iraq Halts Oil Exports from Main Southern Pipel...             0          0   \n",
       "Oil prices soar to all-time record, posing new ...             0          0   \n",
       "\n",
       "                                                    Quizzing  'offensive'  \\\n",
       "title                                                                       \n",
       "Wall St. Bears Claw Back Into the Black (Reuters)          0            0   \n",
       "Carlyle Looks Toward Commercial Aerospace (Reut...         0            0   \n",
       "Oil and Economy Cloud Stocks' Outlook (Reuters)            0            0   \n",
       "Iraq Halts Oil Exports from Main Southern Pipel...         0            0   \n",
       "Oil prices soar to all-time record, posing new ...         0            0   \n",
       "\n",
       "                                                    Barack  Obama  Rauffer  \\\n",
       "title                                                                        \n",
       "Wall St. Bears Claw Back Into the Black (Reuters)        0      0        0   \n",
       "Carlyle Looks Toward Commercial Aerospace (Reut...       0      0        0   \n",
       "Oil and Economy Cloud Stocks' Outlook (Reuters)          0      0        0   \n",
       "Iraq Halts Oil Exports from Main Southern Pipel...       0      0        0   \n",
       "Oil prices soar to all-time record, posing new ...       0      0        0   \n",
       "\n",
       "                                                    Shivering  \n",
       "title                                                          \n",
       "Wall St. Bears Claw Back Into the Black (Reuters)           0  \n",
       "Carlyle Looks Toward Commercial Aerospace (Reut...          0  \n",
       "Oil and Economy Cloud Stocks' Outlook (Reuters)             0  \n",
       "Iraq Halts Oil Exports from Main Southern Pipel...          0  \n",
       "Oil prices soar to all-time record, posing new ...          0  \n",
       "\n",
       "[5 rows x 71744 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm.index= dat['title'].iloc[0:100]\n",
    "tdm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks pretty similar to the isir case. Can we start using a log reg to train and predict----YES we can.\n",
    "\n",
    "### we can say that the machine is understanding language(to some extent)\n",
    "\n",
    "### Lets try and improve the understanding for this case!\n",
    "\n",
    "### 100 observations and 71744 features, it is alot? \n",
    "### we clean!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of text cleaning!!\n",
    "\n",
    "- Case sensitive, lower the text\n",
    "- text has alphabets, digits, punctuations and otherwise special characters, what all to keep?\n",
    "- Extra spaces\n",
    "- To Stopwords or not?\n",
    "- To stem/lemma or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean(sent):\n",
    "    sent=sent.lower()\n",
    "    sent=re.sub(r'[^a-z0-9 ]', \" \",sent)\n",
    "    sebt=re.sub(r'\\s+', \" \",sent)\n",
    "    sent=\" \".join([stemmer.stem(s) for s in sent.split()])\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat['title_clean']= [clean(sent) for sent in dat.description]\n",
    "dat['title_clean']= [clean(sent) for sent in dat.title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Wall St. Bears Claw Back Into the Black (Reuters)\n",
      "1    Carlyle Looks Toward Commercial Aerospace (Reu...\n",
      "2      Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
      "3    Iraq Halts Oil Exports from Main Southern Pipe...\n",
      "4    Oil prices soar to all-time record, posing new...\n",
      "Name: title, dtype: object\n",
      "0         wall st bear claw back into the black reuter\n",
      "1          carlyl look toward commerci aerospac reuter\n",
      "2           oil and economi cloud stock outlook reuter\n",
      "3    iraq halt oil export from main southern pipeli...\n",
      "4    oil price soar to all time record pose new men...\n",
      "Name: title_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dat.title.iloc[0:5])\n",
    "print(dat.title_clean.iloc[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23783"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequency of words\n",
    "words={}\n",
    "for sent in dat.title_clean.values:\n",
    "    for word in sent.split():\n",
    "        if word in words:\n",
    "            words[word]+=1\n",
    "        else:\n",
    "            words[word]=1\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have a good understading of how things function in cleaning and machine understading of the feature vectors. \n",
    "\n",
    "\n",
    "### lets use sklearn proceed !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scikit-Learn's CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a dataset and convert it into a corpus. Then we create a vocabulary of all the unique words in the corpus. Using this vocabulary, we can then create a feature vector of the count of the words. Let's see this through a simple example. Let's say we have a corpus containing two sentences as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#fit\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(dat.title_clean.values)\n",
    "#vectorizer.vocabulary_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 23747)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform\n",
    "\n",
    "train_vectors = vectorizer.transform(dat.title_clean.values)\n",
    "\n",
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a scikit learn Count vectorizer can perform the following opertions over a text corpus:\n",
    "\n",
    "Encoding via utf-8\n",
    "\n",
    "converts text to lowercase\n",
    "\n",
    "Tokenizes text using word level tokenization\n",
    "\n",
    "CountVectorizer has a number of parameters. Let's look at some of them :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "we can remove stopwords depending on our requirements. Can use NLTK or a custom list as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 23629)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "#print(stopwords.words('english'))\n",
    "stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "#fit\n",
    "count_vectorizer = CountVectorizer(stop_words = stopwords)\n",
    "count_vectorizer.fit(dat.title_clean.values)\n",
    "\n",
    "\n",
    "#transform\n",
    "train_vectors = count_vectorizer.transform(dat.title_clean.values)\n",
    "\n",
    "\n",
    "train_vectors.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIN_DF and MAX_DF parameter\n",
    "MIN_DF lets you ignore those terms that appear rarely in a corpus. In other words, if MIN_dfis 2, it means that a word has to occur at least two documents to be considered useful.\n",
    "\n",
    "MAX_DF on the other hand, ignores terms that have a document frequency strictly higher than the given threshold.These will be words which appear a lot of documents.\n",
    "\n",
    "This means we can eliminate those words that are either rare or appear too frequently in a corpus.\n",
    "\n",
    "When mentioned in absolute values i.e 1,2, etc, the value means if the word appears in 1 or 2 documents. However, when given in float, eg 30%, it means it appears in 30% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 15398)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit\n",
    "count_vectorizer = CountVectorizer(stop_words = stopwords, min_df=2 ,max_df=0.8)\n",
    "count_vectorizer.fit(dat.title_clean.values)\n",
    "\n",
    "\n",
    "#transform\n",
    "train_vectors = count_vectorizer.transform(dat.title_clean.values)\n",
    "\n",
    "\n",
    "train_vectors.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom preprocessor\n",
    "\n",
    "Cleaning and other realted tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean(sent):\n",
    "    sent=sent.lower()\n",
    "    sent=re.sub(r'[^A-Za-z ]', \" \",sent)\n",
    "    sent=re.sub(r'\\s+', \" \",sent)\n",
    "    #sent=\" \".join([stemmer.stem(s) for s in sent.split() if s not in stopwords])\n",
    "    \n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 4293)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit\n",
    "count_vectorizer = CountVectorizer( min_df=20 ,max_df=0.8, preprocessor= clean)\n",
    "count_vectorizer.fit(dat.title_clean.values)\n",
    "\n",
    "\n",
    "#transform\n",
    "train_vectors = count_vectorizer.transform(dat.title_clean.values)\n",
    "\n",
    "\n",
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets put our classification model to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= dat.labels.values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "Xtr, Xtt, Ytr, Ytt= train_test_split(train_vectors, y, test_size=.3, stratify= y, random_state=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELECTROBOT\\Anaconda3\\envs\\tarun37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(Xtr, Ytr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= clf.predict(Xtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.11666666666666\n",
      "85.07324842563791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(accuracy_score(Ytt, y_pred)*100)\n",
    "print(f1_score(Ytt, y_pred, average='macro')*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------We have built our first text classification model ---------------\n",
    "\n",
    "Things we learnt here:\n",
    "    \n",
    "    The features are all the words(too many). The word vectors are the count of occurance of thme word(pretty naive).\n",
    "    The feature vector is the combination of all the words.\n",
    "\n",
    "Next steps:\n",
    "    \n",
    "    We will try and improve the word vector quality.(tfidf)\n",
    "    We will try and bring in some sentence structure/contexct information (N Grams)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 2nd approach to text vectorization: TFIDF\n",
    "### For example, when a 100-word document contains the term “cat” 12 times, the TF for the word ‘cat’ is\n",
    "### TFcat = 12/100 i.e. 0.12\n",
    "### Let’s say the size of the corpus is 1000 documents. If we assume there are 30 documents that contain the term “cat”, then\n",
    "### IDF (cat) = log (1000/30) = 1.52\n",
    "### (TF*IDF) cat = 0.12 * 1.52 = 0.182\n",
    "if appears in one document only then its ,1.2*log(1000/1)=3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.11666666666666\n",
      "85.08210738784638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELECTROBOT\\Anaconda3\\envs\\tarun37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(min_df=20 ,max_df=.8, preprocessor= clean)\n",
    "X = vectorizer.fit_transform(dat.title_clean.values)\n",
    "y= dat.labels.values\n",
    "\n",
    "Xtr, Xtt, Ytr, Ytt= train_test_split(X, y, test_size=.3, stratify= y, random_state=98)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(Xtr, Ytr)\n",
    "\n",
    "y_pred= clf.predict(Xtt)\n",
    "\n",
    "print(accuracy_score(Ytt, y_pred)*100)\n",
    "print(f1_score(Ytt, y_pred, average='macro')*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>ababa</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandon its</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandoning its</th>\n",
       "      <th>...</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone and</th>\n",
       "      <th>zone in</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zook</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zvonareva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 40557 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aapl  aaron   ab  ababa  abandon  abandon its  abandoned  \\\n",
       "0    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "1    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "2    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "3    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "4    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "..   ...   ...    ...  ...    ...      ...          ...        ...   \n",
       "295  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "296  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "297  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "298  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "299  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "\n",
       "     abandoning  abandoning its  ...  zimbabwe  zimbabwean  zombie  zone  \\\n",
       "0           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "1           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "2           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "3           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "4           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "..          ...             ...  ...       ...         ...     ...   ...   \n",
       "295         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "296         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "297         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "298         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "299         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "\n",
       "     zone and  zone in  zoo  zook  zurich  zvonareva  \n",
       "0         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "1         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "2         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "3         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "4         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "..        ...      ...  ...   ...     ...        ...  \n",
       "295       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "296       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "297       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "298       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "299       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "\n",
       "[300 rows x 40557 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X[0:300].todense(), columns= vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imporove it more, bring some sentence structure using N-grams\n",
    "\n",
    "sentence1= \"man bites dog\"\n",
    "sentence2= \"dog bites man\"\n",
    "Features extracted will be:['man', 'bites', 'dog']\n",
    "\n",
    "the sentence vector is same using above methods!\n",
    "\n",
    "using 2-gram\n",
    "\n",
    "Features extracted will be:\n",
    "sentence1= ['man', 'bites', 'dog', 'man bites', 'bites dog']\n",
    "sentence1= ['dog', 'bites', 'man', 'dog bites', 'bites man']\n",
    "\n",
    "the sentence vectors are pretty different and can convey quite a bit of information as well!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.51666666666667\n",
      "90.49714239507367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELECTROBOT\\Anaconda3\\envs\\tarun37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(min_df=20 ,max_df=0.8, preprocessor= clean,\n",
    "                             ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(dat.description)\n",
    "y= dat.labels.values\n",
    "\n",
    "Xtr, Xtt, Ytr, Ytt= train_test_split(X, y, test_size=.3, stratify= y, random_state=98)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(Xtr, Ytr)\n",
    "\n",
    "y_pred= clf.predict(Xtt)\n",
    "\n",
    "print(accuracy_score(Ytt, y_pred)*100)\n",
    "print(f1_score(Ytt, y_pred, average='macro')*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>ababa</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandon its</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandoning its</th>\n",
       "      <th>...</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone and</th>\n",
       "      <th>zone in</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zook</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zvonareva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 40557 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aapl  aaron   ab  ababa  abandon  abandon its  abandoned  \\\n",
       "0    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "1    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "2    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "3    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "4    0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "..   ...   ...    ...  ...    ...      ...          ...        ...   \n",
       "295  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "296  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "297  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "298  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "299  0.0   0.0    0.0  0.0    0.0      0.0          0.0        0.0   \n",
       "\n",
       "     abandoning  abandoning its  ...  zimbabwe  zimbabwean  zombie  zone  \\\n",
       "0           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "1           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "2           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "3           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "4           0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "..          ...             ...  ...       ...         ...     ...   ...   \n",
       "295         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "296         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "297         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "298         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "299         0.0             0.0  ...       0.0         0.0     0.0   0.0   \n",
       "\n",
       "     zone and  zone in  zoo  zook  zurich  zvonareva  \n",
       "0         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "1         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "2         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "3         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "4         0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "..        ...      ...  ...   ...     ...        ...  \n",
       "295       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "296       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "297       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "298       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "299       0.0      0.0  0.0   0.0     0.0        0.0  \n",
       "\n",
       "[300 rows x 40557 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X[0:300].todense(), columns= vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we have built our text classification model 2\n",
    "### we did some improvements to the word vectors by using tfidf and we induced some sentence structure using n grams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How can we improve more?\n",
    "\n",
    "### - We have to imrove the sentence vector so that it conveys more information\n",
    "### - To imrpove the sentence vector we have to improve the word vectors.\n",
    "\n",
    "## Lets leave tfidf and move towards word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embeddings\n",
    "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open(r'C:\\Users\\ELECTROBOT\\PycharmProjects\\bot\\tarun_nlp\\glove.6B.50d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going back to our cat example.\n",
    "\"Cat\" appears in our 100 word document 12 times\n",
    "And in 10,000,000 million documents, 0.3 million documents contain the term “cat”\n",
    "\n",
    "Count vector = 12\n",
    "tfidf= 1.52\n",
    "\n",
    "Glove embedding is below and it is global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45281 , -0.50108 , -0.53714 , -0.015697,  0.22191 ,  0.54602 ,\n",
       "       -0.67301 , -0.6891  ,  0.63493 , -0.19726 ,  0.33685 ,  0.7735  ,\n",
       "        0.90094 ,  0.38488 ,  0.38367 ,  0.2657  , -0.08057 ,  0.61089 ,\n",
       "       -1.2894  , -0.22313 , -0.61578 ,  0.21697 ,  0.35614 ,  0.44499 ,\n",
       "        0.60885 , -1.1633  , -1.1579  ,  0.36118 ,  0.10466 , -0.78325 ,\n",
       "        1.4352  ,  0.18629 , -0.26112 ,  0.83275 , -0.23123 ,  0.32481 ,\n",
       "        0.14485 , -0.44552 ,  0.33497 , -0.95946 , -0.097479,  0.48138 ,\n",
       "       -0.43352 ,  0.69455 ,  0.91043 , -0.28173 ,  0.41637 , -1.2609  ,\n",
       "        0.71278 ,  0.23782 ], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.25324005,  1.20732398, -1.81572501,  1.39300997,  2.79697999,\n",
       "        1.55685998, -0.82648612, -1.32325099,  2.159186  , -0.05694999,\n",
       "        0.308964  , -0.238901  , -2.49635896, -0.57219598,  1.67374601,\n",
       "        1.06874001,  0.45898998, -0.33058   , -2.65711996, -1.22397998,\n",
       "       -0.83847098,  0.84496   ,  0.31227602, -0.44233698,  1.31393898,\n",
       "       -9.48469996, -1.76770702,  1.69573998, -0.85943398,  0.55382499,\n",
       "       17.62400007, -1.80384001, -2.21263   , -1.69688997,  1.26198198,\n",
       "       -1.53818896,  2.98966001,  0.83914001, -0.74335   , -1.17333001,\n",
       "        0.17548701,  0.58401601, -1.65164995, -0.11220033,  0.13959499,\n",
       "        1.49597199, -0.21923009,  2.85804805,  1.71129   ,  0.95066002])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all all embeddings to create sentence vector\n",
    "\n",
    "sentence= \"this is a test statement\"\n",
    "\n",
    "feature_vec=np.zeros(50)\n",
    "\n",
    "\n",
    "for word in sentence.split():\n",
    "    feature_vec+=embeddings_index[word]\n",
    "feature_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec=[]\n",
    "for sent in dat.description:\n",
    "    sent= sent.lower()\n",
    "    sent= re.sub(r'[^a-z ]', \" \", sent)\n",
    "    sent= re.sub(r'\\s+', \" \", sent)\n",
    "    sent= \" \".join([i for i in sent.split() if i not in stopwords])\n",
    "    sent_vec=np.zeros(50)\n",
    "    counter=1\n",
    "    #print(sent)\n",
    "    for word in sent.split():\n",
    "        try:\n",
    "            sent_vec+=embeddings_index[word]\n",
    "            counter+=1\n",
    "        except:\n",
    "            pass\n",
    "    all_vec.append(sent_vec/counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec=np.vstack(all_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.01388888888889\n",
      "87.00148423579151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELECTROBOT\\Anaconda3\\envs\\tarun37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "Xtr, Xtt, Ytr, Ytt= train_test_split(all_vec, y, test_size=.3, stratify= y, random_state=98)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(Xtr, Ytr)\n",
    "\n",
    "y_pred= clf.predict(Xtt)\n",
    "\n",
    "print(accuracy_score(Ytt, y_pred)*100)\n",
    "print(f1_score(Ytt, y_pred, average='macro')*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08744174,  0.05374192,  0.40379584, ..., -0.218543  ,\n",
       "        -0.04069583, -0.04449614],\n",
       "       [ 0.18624144, -0.10160886,  0.02472765, ...,  0.29362154,\n",
       "         0.07653359,  0.17087973],\n",
       "       [-0.08487244, -0.04802153,  0.55133232, ...,  0.4701391 ,\n",
       "         0.25072272,  0.08087132],\n",
       "       ...,\n",
       "       [-0.16266912,  0.05424985,  0.09967032, ...,  0.47202923,\n",
       "        -0.08021916,  0.27775008],\n",
       "       [-0.44951438,  0.18696011,  0.29057535, ...,  0.32472168,\n",
       "        -0.07712095,  0.11204817],\n",
       "       [-0.59949935,  0.45124391,  0.03657286, ..., -0.13639747,\n",
       "        -0.31487774,  0.27047695]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tarun37]",
   "language": "python",
   "name": "conda-env-tarun37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
